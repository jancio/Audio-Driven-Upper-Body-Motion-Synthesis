{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Audio-driven upper-body motion synthesis on a humanoid robot\n",
    "# Computer Science Tripos Part III Project\n",
    "# Jan Ondras (jo356@cam.ac.uk), Trinity College, University of Cambridge\n",
    "# 2017/18\n",
    "#####################################################################################\n",
    "# 10-fold subject-independent cross-validation of the MLP-SI model\n",
    "# Using the MLP-SI model, train&test on 10 folds (every time 2 test subjects and 2 validation subjects) \n",
    "# Then save the results by subjects. \n",
    "# 1. cell - evaluate on validation partition\n",
    "# 2. cell - evaluate on testing partition\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Evaluate on validation partition\n",
    "###############################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from geoutils import radToDeg, degToRad\n",
    "from evalutils import norm_Y, inv_norm_Y\n",
    "from postprocessingutils import save_predictions_and_eval2\n",
    "\n",
    "N_folds = 10\n",
    "\n",
    "SEGMENT_LEN = 300 # for evaluation (local cca)\n",
    "\n",
    "np.random.seed(37) # for reproducibility\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "unique_srt_VIDs = unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs'] # sorted VIDs\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "###\n",
    "# Learning settings\n",
    "epochs = 1000\n",
    "dropout = 0.\n",
    "\n",
    "########################################################################\n",
    "# For given subject SID (PID02) get corresponding VIDs\n",
    "def get_subjects_VIDs(SIDs):\n",
    "    result_VIDs = []\n",
    "    for SID in SIDs:\n",
    "        # Take VIDs from both tasks, for this subject\n",
    "        result_VIDs.append(  SID + 'Task2' )\n",
    "        result_VIDs.append(  SID + 'Task3' )\n",
    "    return result_VIDs\n",
    "\n",
    "# For given subjects get corresponding indicies into the feature set & also their counts (per VID)\n",
    "def get_subjects_indicies(SIDs):\n",
    "    indicies = []\n",
    "    indicies_cnts = []\n",
    "    for SID in SIDs:\n",
    "        # Take indicies corresponding to both tasks, for this subject\n",
    "        a = np.argwhere(all_srt_VIDs == SID + 'Task2')[:,0]\n",
    "        b = np.argwhere(all_srt_VIDs == SID + 'Task3')[:,0]\n",
    "        indicies.extend( a )\n",
    "        indicies.extend( b )\n",
    "        indicies_cnts.append( len(a) )\n",
    "        indicies_cnts.append( len(b) )\n",
    "    return indicies, indicies_cnts\n",
    "# print frameCnts[26] + frameCnts[27] + frameCnts[0] + frameCnts[1] # checks\n",
    "# print len(get_subjects_indicies(['PID20', 'PID02']))\n",
    "########################################################################\n",
    "\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SI/cvVal/'\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/MLP_SI/cvVal/'\n",
    "\n",
    "PF = np.load('./../Dataset/'+TE_folder+'/te_PF_smooth_LPBF_4.0.npz')['PF_smooth_LPBF']\n",
    "PF = PF[:, :11]\n",
    "N_targets = PF.shape[1]\n",
    "###########\n",
    "# Target (Y) normalisation, into range 0-1 according to constraints\n",
    "PF = norm_Y(PF)\n",
    "print \"Targets (Y) are TRANSFORMED to 0-1 range\"\n",
    "\n",
    "tuning_types = [\n",
    "    '1_35_AF13',\n",
    "    '1_35_AF26', \n",
    "    '1_35_AF52', \n",
    "    '1_35_AF78'\n",
    "]\n",
    "AF_types = [\n",
    "    'AF_MFCC13_norm',\n",
    "    'AF_logFB26_norm',\n",
    "    'AF_logFB52_norm',\n",
    "    'AF_logFB78_norm'\n",
    "]\n",
    "\n",
    "for tuning_type, AF_type in zip(tuning_types, AF_types):\n",
    "    \n",
    "    print AF_type, tuning_type\n",
    "\n",
    "    AF = np.load('./../Dataset/'+TE_folder+'/te_'+AF_type+'.npz')[AF_type]\n",
    "    N_features = AF.shape[1]\n",
    "\n",
    "    #######################\n",
    "    # Load validation data\n",
    "    dd = np.load('./../Dataset/'+TE_folder+'/Results/MLP_SI/XXXval_' + tuning_type + '.npz')\n",
    "    best_N_hl = int(dd['best_N_hl'])\n",
    "    best_N_hu = int(dd['best_N_hu'])    \n",
    "    print \"\\tOptimal number of hidden layers / hidden units: \", best_N_hl, \" / \", best_N_hu\n",
    "\n",
    "    #######################################################\n",
    "    # Dataset split\n",
    "\n",
    "    N_test_SIDs = 2\n",
    "    N_val_SIDs = 2\n",
    "    N_SIDs = 19\n",
    "    N_train_SIDs = N_SIDs - N_test_SIDs - N_val_SIDs\n",
    "    print \"Dataset split in terms of subjects (train/val/test): \", 100.*N_train_SIDs/N_SIDs, \"/\", 100.*N_val_SIDs/N_SIDs, \"/\", 100.*N_test_SIDs/N_SIDs, \"%\"\n",
    "    print \n",
    "\n",
    "    # Randomise the dataset split\n",
    "    permI = np.random.permutation(N_SIDs)\n",
    "\n",
    "    for fold_i in range(N_folds):\n",
    "\n",
    "        print permI\n",
    "        train_SIDs_mask = permI[:N_train_SIDs] \n",
    "        val_SIDs_mask =   permI[N_train_SIDs:N_train_SIDs+N_val_SIDs]\n",
    "        test_SIDs_mask =  permI[N_train_SIDs+N_val_SIDs:] \n",
    "\n",
    "        print \"Train SIDs\",      unique_srt_SIDs[train_SIDs_mask]\n",
    "        print \"Valid SIDs\",      unique_srt_SIDs[val_SIDs_mask]\n",
    "        print \"Testi SIDs\",      unique_srt_SIDs[test_SIDs_mask]\n",
    "\n",
    "        print \"Train SIDs mask\", train_SIDs_mask\n",
    "        print \"Valid SIDs mask\", val_SIDs_mask\n",
    "        print \"Testi SIDs mask\", test_SIDs_mask\n",
    "\n",
    "        train_VIDs = get_subjects_VIDs(unique_srt_SIDs[train_SIDs_mask])\n",
    "        val_VIDs   = get_subjects_VIDs(unique_srt_SIDs[val_SIDs_mask])\n",
    "        test_VIDs  = get_subjects_VIDs(unique_srt_SIDs[test_SIDs_mask])\n",
    "\n",
    "        print \"Train VIDs\", train_VIDs\n",
    "        print \"Valid VIDs\", val_VIDs\n",
    "        print \"Testi VIDs\", test_VIDs\n",
    "\n",
    "        train_mask, train_VIDs_ind_cnts = get_subjects_indicies(unique_srt_SIDs[train_SIDs_mask])\n",
    "        val_mask, val_VIDs_ind_cnts     = get_subjects_indicies(unique_srt_SIDs[val_SIDs_mask])\n",
    "        test_mask, test_VIDs_ind_cnts   = get_subjects_indicies(unique_srt_SIDs[test_SIDs_mask])\n",
    "        # print test_VIDs_ind_cnts\n",
    "\n",
    "        print \"Train/val/test set sizes: \", len(train_mask), \"/\", len(val_mask), \"/\", len(test_mask), \" = \", len(train_mask) + len(val_mask) + len(test_mask)\n",
    "        print \"Dataset split in terms of #examples (train/val/test): \", 100.*len(train_mask)/len(all_srt_VIDs), \"/\", 100.*len(val_mask)/len(all_srt_VIDs), \"/\", 100.*len(test_mask)/len(all_srt_VIDs), \"%\"\n",
    "        print \n",
    "\n",
    "        permI = np.roll(permI, 2) # ROTATE INDICES - FOR NEXT DATASET SPLIT\n",
    "\n",
    "        #######################################################\n",
    "        # Training\n",
    "\n",
    "        Y_train = PF[train_mask]\n",
    "        Y_val   = PF[val_mask]\n",
    "        #Y_test  = PF[test_mask]\n",
    "        X_train = AF[train_mask]\n",
    "        X_val   = AF[val_mask]\n",
    "        #X_test  = AF[test_mask]\n",
    "        train_batch_size = len(X_train)\n",
    "        val_batch_size = len(X_val)\n",
    "        #test_batch_size = len(X_test)\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        ##########################\n",
    "        # Final train & test\n",
    "\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform', input_dim=N_features))\n",
    "        #model.add(Dropout(dropout))\n",
    "        for _ in range(1, best_N_hl):\n",
    "            model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform'))\n",
    "            #model.add(Dropout(dropout))\n",
    "        model.add(Dense(N_targets, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "        #print model.summary()\n",
    "        #print \"#parameters: \", model.count_params()\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1) # stop after 10 epochs without improvement in val_acc\n",
    "\n",
    "        # Checkpoint model weights and the model itself: at each epoch\n",
    "        model_checkpoint_name = 'm_{:}_{:02d}'.format(tuning_type, i) + '_{epoch:04d}_{loss:.4f}_{val_loss:.4f}.hdf5'\n",
    "        model_checkpoint = ModelCheckpoint(model_checkpoint_path_prefix + model_checkpoint_name, monitor='val_loss', \n",
    "                                           verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "        hist = model.fit(X_train, Y_train, epochs=epochs, batch_size=train_batch_size, \n",
    "                   validation_data = (X_val, Y_val), verbose=0, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "        #######################################################\n",
    "        # Testing & evaluation & saving results\n",
    "\n",
    "        Y_val_pred = model.predict(X_val, batch_size=val_batch_size, verbose=1)\n",
    "\n",
    "        # 1. val subject\n",
    "        cnt = val_VIDs_ind_cnts[0] + val_VIDs_ind_cnts[1]\n",
    "\n",
    "        # Avoid evaluation of PID20 twice\n",
    "        if fold_i == N_folds - 1 and val_VIDs[0][:5] == 'PID20':\n",
    "            print \"Avoiding evaluation of PID20 twice!\"\n",
    "        else:\n",
    "            save_predictions_and_eval2(save_results_path_prefix + 'val_' + tuning_type + '_' + val_VIDs[0][:5], \n",
    "                            X_val[:cnt], Y_val[:cnt], Y_val_pred[:cnt], 'MLP_SI', SEGMENT_LEN, val_VIDs[:2], \n",
    "                            val_VIDs_ind_cnts[:2], N_params=model.count_params(), N_epochs=len(hist.history['loss']))    \n",
    "\n",
    "        # 2. val subject\n",
    "        save_predictions_and_eval2(save_results_path_prefix + 'val_' + tuning_type + '_' + val_VIDs[2][:5], \n",
    "                        X_val[cnt:], Y_val[cnt:], Y_val_pred[cnt:], 'MLP_SI', SEGMENT_LEN, val_VIDs[2:], \n",
    "                        val_VIDs_ind_cnts[2:], N_params=model.count_params(), N_epochs=len(hist.history['loss']))    \n",
    "\n",
    "        print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Evaluate on testing partition\n",
    "###############################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from geoutils import radToDeg, degToRad\n",
    "from evalutils import norm_Y, inv_norm_Y\n",
    "from postprocessingutils import save_predictions_and_eval2\n",
    "\n",
    "N_folds = 10\n",
    "\n",
    "SEGMENT_LEN = 300 # for evaluation (local cca)\n",
    "AF_type = 'AF_logFB26_norm'\n",
    "\n",
    "np.random.seed(37) # for reproducibility\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "unique_srt_VIDs = unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs'] # sorted VIDs\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "###\n",
    "tuning_type = '1_35_AF26'\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SI/cvTest/'\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/MLP_SI/cvTest/'\n",
    "\n",
    "PF = np.load('./../Dataset/'+TE_folder+'/te_PF_smooth_LPBF_4.0.npz')['PF_smooth_LPBF']\n",
    "PF = PF[:, :11]\n",
    "N_targets = PF.shape[1]\n",
    "###########\n",
    "# Target (Y) normalisation, into range 0-1 according to constraints\n",
    "PF = norm_Y(PF)\n",
    "print \"Targets (Y) are TRANSFORMED to 0-1 range\"\n",
    "\n",
    "AF = np.load('./../Dataset/'+TE_folder+'/te_'+AF_type+'.npz')[AF_type]\n",
    "N_features = AF.shape[1]\n",
    "\n",
    "#######################\n",
    "# Load validation data\n",
    "dd = np.load('./../Dataset/'+TE_folder+'/Results/MLP_SI/XXXval_' + tuning_type + '.npz')\n",
    "best_N_hl = int(dd['best_N_hl'])\n",
    "best_N_hu = int(dd['best_N_hu'])    \n",
    "print \"\\tOptimal number of hidden layers / hidden units: \", best_N_hl, \" / \", best_N_hu\n",
    "\n",
    "\n",
    "# Learning settings\n",
    "epochs = 1000\n",
    "dropout = 0.\n",
    "\n",
    "########################################################################\n",
    "# For given subject SID (PID02) get corresponding VIDs\n",
    "def get_subjects_VIDs(SIDs):\n",
    "    result_VIDs = []\n",
    "    for SID in SIDs:\n",
    "        # Take VIDs from both tasks, for this subject\n",
    "        result_VIDs.append(  SID + 'Task2' )\n",
    "        result_VIDs.append(  SID + 'Task3' )\n",
    "    return result_VIDs\n",
    "\n",
    "# For given subjects get corresponding indicies into the feature set & also their counts (per VID)\n",
    "def get_subjects_indicies(SIDs):\n",
    "    indicies = []\n",
    "    indicies_cnts = []\n",
    "    for SID in SIDs:\n",
    "        # Take indicies corresponding to both tasks, for this subject\n",
    "        a = np.argwhere(all_srt_VIDs == SID + 'Task2')[:,0]\n",
    "        b = np.argwhere(all_srt_VIDs == SID + 'Task3')[:,0]\n",
    "        indicies.extend( a )\n",
    "        indicies.extend( b )\n",
    "        indicies_cnts.append( len(a) )\n",
    "        indicies_cnts.append( len(b) )\n",
    "    return indicies, indicies_cnts\n",
    "# print frameCnts[26] + frameCnts[27] + frameCnts[0] + frameCnts[1] # checks\n",
    "# print len(get_subjects_indicies(['PID20', 'PID02']))\n",
    "########################################################################\n",
    "\n",
    "#######################################################\n",
    "# Dataset split\n",
    "\n",
    "N_test_SIDs = 2\n",
    "N_val_SIDs = 2\n",
    "N_SIDs = 19\n",
    "N_train_SIDs = N_SIDs - N_test_SIDs - N_val_SIDs\n",
    "print \"Dataset split in terms of subjects (train/val/test): \", 100.*N_train_SIDs/N_SIDs, \"/\", 100.*N_val_SIDs/N_SIDs, \"/\", 100.*N_test_SIDs/N_SIDs, \"%\"\n",
    "print \n",
    "\n",
    "# Randomise the dataset split\n",
    "permI = np.random.permutation(N_SIDs)\n",
    "\n",
    "for fold_i in range(N_folds):\n",
    "    \n",
    "    print permI\n",
    "    train_SIDs_mask = permI[:N_train_SIDs] \n",
    "    val_SIDs_mask =   permI[N_train_SIDs:N_train_SIDs+N_val_SIDs]\n",
    "    test_SIDs_mask =  permI[N_train_SIDs+N_val_SIDs:] \n",
    "    \n",
    "    print \"Train SIDs\",      unique_srt_SIDs[train_SIDs_mask]\n",
    "    print \"Valid SIDs\",      unique_srt_SIDs[val_SIDs_mask]\n",
    "    print \"Testi SIDs\",      unique_srt_SIDs[test_SIDs_mask]\n",
    "\n",
    "    print \"Train SIDs mask\", train_SIDs_mask\n",
    "    print \"Valid SIDs mask\", val_SIDs_mask\n",
    "    print \"Testi SIDs mask\", test_SIDs_mask\n",
    "\n",
    "    train_VIDs = get_subjects_VIDs(unique_srt_SIDs[train_SIDs_mask])\n",
    "    val_VIDs   = get_subjects_VIDs(unique_srt_SIDs[val_SIDs_mask])\n",
    "    test_VIDs  = get_subjects_VIDs(unique_srt_SIDs[test_SIDs_mask])\n",
    "\n",
    "    print \"Train VIDs\", train_VIDs\n",
    "    print \"Valid VIDs\", val_VIDs\n",
    "    print \"Testi VIDs\", test_VIDs\n",
    "\n",
    "    train_mask, train_VIDs_ind_cnts = get_subjects_indicies(unique_srt_SIDs[train_SIDs_mask])\n",
    "    val_mask, val_VIDs_ind_cnts     = get_subjects_indicies(unique_srt_SIDs[val_SIDs_mask])\n",
    "    test_mask, test_VIDs_ind_cnts   = get_subjects_indicies(unique_srt_SIDs[test_SIDs_mask])\n",
    "    # print test_VIDs_ind_cnts\n",
    "\n",
    "    print \"Train/val/test set sizes: \", len(train_mask), \"/\", len(val_mask), \"/\", len(test_mask), \" = \", len(train_mask) + len(val_mask) + len(test_mask)\n",
    "    print \"Dataset split in terms of #examples (train/val/test): \", 100.*len(train_mask)/len(all_srt_VIDs), \"/\", 100.*len(val_mask)/len(all_srt_VIDs), \"/\", 100.*len(test_mask)/len(all_srt_VIDs), \"%\"\n",
    "    print \n",
    "\n",
    "    permI = np.roll(permI, 2) # ROTATE INDICES - FOR NEXT DATASET SPLIT\n",
    "\n",
    "    #######################################################\n",
    "    # Training\n",
    "\n",
    "    Y_train = PF[train_mask]\n",
    "    Y_val   = PF[val_mask]\n",
    "    Y_test  = PF[test_mask]\n",
    "    X_train = AF[train_mask]\n",
    "    X_val   = AF[val_mask]\n",
    "    X_test  = AF[test_mask]\n",
    "    train_batch_size = len(X_train)\n",
    "    val_batch_size = len(X_val)\n",
    "    test_batch_size = len(X_test)\n",
    "    \n",
    "    st = time.time()\n",
    "\n",
    "    ##########################\n",
    "    # Final train & test\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform', input_dim=N_features))\n",
    "    #model.add(Dropout(dropout))\n",
    "    for _ in range(1, best_N_hl):\n",
    "        model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform'))\n",
    "        #model.add(Dropout(dropout))\n",
    "    model.add(Dense(N_targets, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    #print model.summary()\n",
    "    #print \"#parameters: \", model.count_params()\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1) # stop after 10 epochs without improvement in val_acc\n",
    "\n",
    "    # Checkpoint model weights and the model itself: at each epoch\n",
    "    model_checkpoint_name = 'm_{:02d}'.format(i) + '_{epoch:04d}_{loss:.4f}_{val_loss:.4f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_checkpoint_path_prefix + model_checkpoint_name, monitor='val_loss', \n",
    "                                       verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    hist = model.fit(X_train, Y_train, epochs=epochs, batch_size=train_batch_size, \n",
    "               validation_data = (X_val, Y_val), verbose=0, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "    #######################################################\n",
    "    # Testing & evaluation & saving results\n",
    "    \n",
    "    Y_test_pred = model.predict(X_test, batch_size=test_batch_size, verbose=1)\n",
    "    \n",
    "    # 1. test subject\n",
    "    cnt = test_VIDs_ind_cnts[0] + test_VIDs_ind_cnts[1]\n",
    "    \n",
    "    # Avoid evaluation of PID23 twice\n",
    "    if fold_i == N_folds - 1 and test_VIDs[0][:5] == 'PID23':\n",
    "        print \"Avoiding evaluation of PID23 twice!\"\n",
    "    else:\n",
    "        save_predictions_and_eval2(save_results_path_prefix + 'test_' + test_VIDs[0][:5], \n",
    "                        X_test[:cnt], Y_test[:cnt], Y_test_pred[:cnt], 'MLP_SI', SEGMENT_LEN, test_VIDs[:2], \n",
    "                        test_VIDs_ind_cnts[:2], N_params=model.count_params(), N_epochs=len(hist.history['loss']))    \n",
    "\n",
    "    # 2. test subject\n",
    "    save_predictions_and_eval2(save_results_path_prefix + 'test_' + test_VIDs[2][:5], \n",
    "                    X_test[cnt:], Y_test[cnt:], Y_test_pred[cnt:], 'MLP_SI', SEGMENT_LEN, test_VIDs[2:], \n",
    "                    test_VIDs_ind_cnts[2:], N_params=model.count_params(), N_epochs=len(hist.history['loss']))    \n",
    "\n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
