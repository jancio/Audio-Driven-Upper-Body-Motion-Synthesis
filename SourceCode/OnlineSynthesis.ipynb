{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Audio-driven upper-body motion synthesis on a humanoid robot\n",
    "# Computer Science Tripos Part III Project\n",
    "# Jan Ondras (jo356@cam.ac.uk), Trinity College, University of Cambridge\n",
    "# 2017/18\n",
    "#####################################################################################\n",
    "# This performs online synthesis on the robot and involves the following steps:\n",
    "#         Load model\n",
    "#         In loop do:\n",
    "#             Read audio frames\n",
    "#             Extract audio features\n",
    "#             Z-norm\n",
    "#             Predict\n",
    "#             Convert to radians (inversion)\n",
    "#             Apply Kalman filter\n",
    "#             Send commands to the robot\n",
    "#         Save recorded audio\n",
    "#         + Report latency measurements\n",
    "\n",
    "# Before starting\n",
    "# 1.) run NaoQi framework\n",
    "# 2.) connect to the real robot OR start Choregraphe simulator and connect to the virtual robot\n",
    "\n",
    "# Before starting the Jupyter Notebook you may need to run the following from the directory you start the Jupyter Notebook: \n",
    "# export PYTHONPATH=${PYTHONPATH}:~/Desktop/pynaoqi-python2.7-2.5.5.5-linux64/lib/python2.7/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# BEGIN SETTINGS\n",
    "\n",
    "# Where to save the captured audio\n",
    "AUDIO_OUTPUT_FILENAME = './Demo/online_audio.wav'\n",
    "\n",
    "# Select pose regression model to use\n",
    "model_type = 'MLP_SI'\n",
    "# model_type = 'LSTM_SI'\n",
    "\n",
    "# Specify IP and port to connect to the robot\n",
    "IP = \"127.0.0.1\"\n",
    "port = 44743\n",
    "IP = 'pepper.local' # real robot settings\n",
    "port = 9559\n",
    "\n",
    "# Duration of simultaneous recording and synthesis\n",
    "MAX_RECORD_SECONDS = 100.\n",
    "\n",
    "# END SETTINGS\n",
    "######################################\n",
    "\n",
    "import wave\n",
    "from naoqi import ALProxy\n",
    "import numpy as np\n",
    "import time\n",
    "from evalutils import inv_norm_Y_vec\n",
    "from python_speech_features import mfcc, delta, logfbank\n",
    "from KFClass import KFOnline\n",
    "\n",
    "SEGMENT_LEN = 300\n",
    "\n",
    "# Load z-normalisation parameters: to be applied on-the-fly (calculated over the whole original dataset)\n",
    "znorm_data = np.load('./../SourceCode/Znorm/znorm_params.npz')\n",
    "znorm_mean = znorm_data['mean']\n",
    "znorm_std  = znorm_data['std']\n",
    "\n",
    "FR = 100. # frame rate of generated pose features\n",
    "dt = 1./FR\n",
    "\n",
    "angles_names = [\n",
    "    \"HeadPitch\", \"HeadYaw\", \n",
    "    \"LShoulderRoll\", \"LShoulderPitch\", \"LElbowRoll\", \"LElbowYaw\",\n",
    "    \"RShoulderRoll\", \"RShoulderPitch\", \"RElbowRoll\", \"RElbowYaw\", \n",
    "    \"HipRoll\"\n",
    "]\n",
    "N_features = 26\n",
    "N_targets = len(angles_names)\n",
    "angles_used_i = np.arange(N_targets)\n",
    "\n",
    "# Connect to the robot\n",
    "motionProxy = ALProxy(\"ALMotion\", IP, port)\n",
    "# Reset robot to neutral pose\n",
    "for an in angles_names:\n",
    "    angle_reset = 0.\n",
    "    if an == 'LShoulderPitch' or an == 'RShoulderPitch':\n",
    "        angle_reset = angle_reset + np.pi/2\n",
    "    motionProxy.setAngles(an, angle_reset, 1.)\n",
    "    \n",
    "##################################################################\n",
    "# Setup Kalman filter, for each angle independently\n",
    "# if filter_predictions:\n",
    "# Initialize Kalman Filter; without constraints\n",
    "KF_list = [] # list Kalman Filter objects for each angle\n",
    "for i in range(N_targets):\n",
    "    init_angle = 0.           # set initial position\n",
    "    if angles_names[i] == 'LShoulderPitch' or angles_names[i] == 'RShoulderPitch':\n",
    "        init_angle = np.pi/2\n",
    "    KF_list.append(\n",
    "        KFOnline(dt=dt, q=0.5, r=0.01, init_P_post=np.matrix('1 0 0; 0 1 0; 0 0 1'), \n",
    "                 init_x_est_post=np.matrix(str(init_angle) + '; 0; 0'))\n",
    "    )\n",
    "\n",
    "##################################################################\n",
    "# Load model\n",
    "from keras.models import load_model\n",
    "model_path_prefix = './../Models/'\n",
    "test_model_name = model_path_prefix + model_type + '/' + model_type + '_model.hdf5'\n",
    "model = load_model(test_model_name)\n",
    "print \"MODEL = \", model_type, \"\\n\"\n",
    "print model.summary()\n",
    "\n",
    "\n",
    "##################################################################\n",
    "latencies_inference = [] # model inference\n",
    "latencies_ops = []       # all other per-frame operations, except model inference\n",
    "latencies_all = []       # all per-frame operations\n",
    "\n",
    "audio_frames = []\n",
    "raw_audio_frames = [] # required to store audio afterwards\n",
    "pose_frames = []\n",
    "pose_frames_filt = []\n",
    "angles_vector_filt = np.zeros(N_targets) # temporary vector of filtered angles (updated each time-step)\n",
    "whole_audio_data = [] # stores int16 audio data, incrementally appended\n",
    "\n",
    "# Initialise logged audio frames with zero vectors (avoids zero-padding in real-time)\n",
    "for i in range(SEGMENT_LEN):\n",
    "    audio_frames.append( np.zeros(N_features) )\n",
    "    \n",
    "# First prediction - just to test (and the first one takes longer => avoids real-time delay)\n",
    "if model_type == 'MLP_SI':\n",
    "    audio_frames_SEGMENT = np.reshape(audio_frames[-1], (1, N_features)) # 1x26 matrix to make prediction on\n",
    "else:\n",
    "    audio_frames_SEGMENT = np.reshape(audio_frames, (1, SEGMENT_LEN, N_features)) # 1x300x26 matrix to make prediction on\n",
    "\n",
    "model.predict(audio_frames_SEGMENT, batch_size=1, verbose=1) #[0,-1] \n",
    "print \"First test prediction done. \"\n",
    "\n",
    "##################################################################\n",
    "# Setup & start audio capture\n",
    "# http://people.csail.mit.edu/hubert/pyaudio/\n",
    "# OR: https://python-sounddevice.readthedocs.io/en/0.3.10/examples.html\n",
    "import pyaudio\n",
    "AUDIO_CHANNELS = 1\n",
    "AUDIO_RATE = 16000 #44100\n",
    "AUDIO_CHUNK = int(float(AUDIO_RATE) / FR) # number of audio samples read in one iteration; AUDIO_RATE / FR is maximum\n",
    "AUDIO_FORMAT = pyaudio.paInt16 \n",
    "N_audio_chunks = int(MAX_RECORD_SECONDS * FR) # int(MAX_RECORD_SECONDS * AUDIO_RATE / AUDIO_CHUNK)\n",
    "LAST_N_AUDIO_DATAPOINTS_FOR_AF_EXTRACTION = int(0.025*AUDIO_RATE) #= 400 # to extract features from one last window\n",
    "print \"AUDIO_CHUNK = \", AUDIO_CHUNK, \"\\t N_audio_chunks = \", N_audio_chunks\n",
    "\n",
    "print(\"************* Recording *************\")\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=AUDIO_FORMAT, #pyaudio.paInt16, #p.get_format_from_width(WIDTH),\n",
    "                channels=AUDIO_CHANNELS,\n",
    "                rate=AUDIO_RATE,\n",
    "                input=True, #output=True,\n",
    "                frames_per_buffer=AUDIO_CHUNK)\n",
    "    \n",
    "st = time.time()\n",
    "for i in range(N_audio_chunks):\n",
    "    \n",
    "    ##############################\n",
    "    # Start latency measurement - for latencies_all\n",
    "    lm_start = time.time() \n",
    "\n",
    "    ##############################\n",
    "    # Read AUDIO_CHUNK audio samples\n",
    "    raw_audio_data = stream.read(AUDIO_CHUNK)\n",
    "    raw_audio_frames.append( raw_audio_data ) # required to store audio afterwards\n",
    "    audio_data = np.fromstring(raw_audio_data , dtype=np.int16) # array of 160 ints\n",
    "    whole_audio_data.extend( audio_data ) # whole audio stream (ints, not raw)\n",
    "        \n",
    "    ##############################\n",
    "    # Extract audio features\n",
    "    # 26 log filter bank features\n",
    "    audio_feature_vector = logfbank(np.array(whole_audio_data[-LAST_N_AUDIO_DATAPOINTS_FOR_AF_EXTRACTION:]),\n",
    "                                    samplerate=AUDIO_RATE,winlen=0.025,winstep=0.01,\n",
    "                                    nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97)[0] # takes the only row\n",
    "    #print audio_feature_vector.shape == (26,)\n",
    "    \n",
    "    ##############################\n",
    "    # Z-norm\n",
    "    audio_feature_vector = (audio_feature_vector - znorm_mean) / znorm_std\n",
    "        \n",
    "    ##############################\n",
    "    # Add new audio feature vector to log (z-normed)\n",
    "    audio_frames.append( audio_feature_vector )\n",
    "\n",
    "    ##############################\n",
    "    # Predict: take last SEGMENT_LEN audio feature vectors if LSTM model (already pre-padded with zerovectors)\n",
    "    lm_inference_start = time.time() # Start inference latency measurement - for latencies_inference\n",
    "    if model_type == 'MLP_SI':\n",
    "        audio_frames_SEGMENT = np.reshape(audio_feature_vector, (1, N_features))         # 1x26 \n",
    "        angles_vector = model.predict(audio_frames_SEGMENT, batch_size=1, verbose=0)[0] \n",
    "    else:\n",
    "        audio_frames_SEGMENT = np.reshape(audio_frames[-SEGMENT_LEN:], (1, SEGMENT_LEN, N_features)) # 1x300x26 matrix to make prediction on\n",
    "        angles_vector = model.predict(audio_frames_SEGMENT, batch_size=1, verbose=0)[0,-1] # take most recent prediction\n",
    "        # [0] since only one sequence is predicted; [-1] only last timestep is taken\n",
    "    lm_inference_stop = time.time() # Stop inference latency measurement - for latencies_inference\n",
    "    \n",
    "    ##############################\n",
    "    # Convert to radians (inversion)\n",
    "    angles_vector = inv_norm_Y_vec(angles_vector)\n",
    "    \n",
    "    ##############################\n",
    "    # Record predictions\n",
    "    pose_frames.append( angles_vector )\n",
    "    \n",
    "    ##############################\n",
    "    # Post Kalman filter\n",
    "    for i in range(N_targets):\n",
    "        # Filter & get estimates\n",
    "        angles_vector_filt[i] = KF_list[i].update(angles_vector[i])[0] # retrieve only angle (not velocity/acceleration)\n",
    "    # Record filtered predictions\n",
    "    pose_frames_filt.append( angles_vector_filt )\n",
    "    \n",
    "    ##############################\n",
    "    # Stop latency measurement\n",
    "    lm_beforeCmd_stop = time.time()\n",
    "    \n",
    "    ##############################\n",
    "    # Send commands to the robot\n",
    "    for angle_i in angles_used_i:\n",
    "        motionProxy.setAngles(angles_names[angle_i], angles_vector_filt[angle_i], 1.)\n",
    "                \n",
    "    ##############################\n",
    "    # Stop latency measurement\n",
    "    lm_stop = time.time()\n",
    "    latencies_inference.append( lm_inference_stop - lm_inference_start )\n",
    "    latencies_all.append( lm_stop - lm_start )\n",
    "        \n",
    "    ##############################\n",
    "    # Adaptive time delay\n",
    "    adaptive_dt = ((i+1.) * dt - time.time() + st)\n",
    "    if adaptive_dt > 0.:\n",
    "        time.sleep(0.95*adaptive_dt)\n",
    "        \n",
    "print(\"************* Done *************\")\n",
    "et = time.time()\n",
    "print \"\\tTotal simulation time: \", et - st, \" s = \", (et - st)/60., \" min; \\t\\tORIGINALLY SET to: \", MAX_RECORD_SECONDS\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate() \n",
    "\n",
    "##############################\n",
    "# Save recorded audio\n",
    "wf = wave.open(AUDIO_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(AUDIO_CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(AUDIO_FORMAT))\n",
    "wf.setframerate(AUDIO_RATE)\n",
    "wf.writeframes(b''.join(raw_audio_frames))\n",
    "wf.close()\n",
    "print \"Saved audio file:\", AUDIO_OUTPUT_FILENAME\n",
    "\n",
    "##############################\n",
    "# Report latency measurements\n",
    "latencies_all = np.array(latencies_all)\n",
    "latencies_inference = np.array(latencies_inference)\n",
    "latencies_ops = latencies_all - latencies_inference\n",
    "\n",
    "print \"Overall:\"\n",
    "print \"Mean +/- std latency: \", 1000*np.mean(latencies_all), 1000*np.std(latencies_all), \"; \\tMax/min: \", np.max(latencies_all), np.min(latencies_all)\n",
    "print \n",
    "print \"Inference:\"\n",
    "print \"Mean +/- std latency: \", 1000*np.mean(latencies_inference), 1000*np.std(latencies_inference), \"; \\tMax/min: \", np.max(latencies_inference), np.min(latencies_inference)\n",
    "print \n",
    "print \"Ops:\"\n",
    "print \"Mean +/- std latency: \", 1000*np.mean(latencies_ops), 1000*np.std(latencies_ops), \"; \\tMax/min: \", np.max(latencies_ops), np.min(latencies_ops)\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
