{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Audio-driven upper-body motion synthesis on a humanoid robot\n",
    "# Computer Science Tripos Part III Project\n",
    "# Jan Ondras (jo356@cam.ac.uk), Trinity College, University of Cambridge\n",
    "# 2017/18\n",
    "#####################################################################################\n",
    "# Training and testing of the MLP-SD model (uses the best architecture found for MLP-SI)\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Baseline MLP (assuming independence between timesteps)\n",
    "# Subject-dependent\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "# FINAL TRAINING \n",
    "# & TESTING on VALIDATION SET (all feature sets)\n",
    "# & TESTING on TESTING SET (best feature set only)\n",
    "# Save trained models\n",
    "# Train a model for each subject separately, using the best architecture for feature set determmined by MLP_SI\n",
    "# No dropout\n",
    "# DONE\n",
    "#######################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from geoutils import radToDeg, degToRad\n",
    "from evalutils import get_global_cca, get_local_cca, eval_test, norm_Y, inv_norm_Y, plot_predictions\n",
    "\n",
    "SEGMENT_LEN = 300 # for evaluation (local cca)\n",
    "\n",
    "tuning_types = [\n",
    "    '1_35_AF13',\n",
    "    '1_35_AF26', \n",
    "    '1_35_AF52', \n",
    "    '1_35_AF78'\n",
    "]\n",
    "AF_types = [\n",
    "    'AF_MFCC13_norm',\n",
    "    'AF_logFB26_norm',\n",
    "    'AF_logFB52_norm',\n",
    "    'AF_logFB78_norm'\n",
    "]\n",
    "\n",
    "np.random.seed(37) # for reproducibility\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SD/XXX'\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/MLP_SD/'\n",
    "\n",
    "unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs']\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "PF = np.load('./../Dataset/'+TE_folder+'/te_PF_smooth_LPBF_4.0.npz')['PF_smooth_LPBF']\n",
    "PF = PF[:, :11]\n",
    "N_targets = PF.shape[1]\n",
    "###########\n",
    "# Target (Y) normalisation, into range 0-1 according to constraints\n",
    "PF = norm_Y(PF)\n",
    "print \"Targets (Y) are TRANSFORMED to 0-1 range\"\n",
    "\n",
    "# Learning settings\n",
    "epochs = 1000\n",
    "dropout = 0.\n",
    "\n",
    "st = time.time()\n",
    "for tuning_type, AF_type in zip(tuning_types, AF_types):\n",
    "    \n",
    "    print TE_folder, AF_type, tuning_type\n",
    "    if tuning_type[-2:] != AF_type.split('_')[1][-2:]:\n",
    "        raise ValueError(\"Tuning type and audio feature type mismatch!\")\n",
    "        \n",
    "        \n",
    "    AF = np.load('./../Dataset/'+TE_folder+'/te_'+AF_type+'.npz')[AF_type]\n",
    "    N_features = AF.shape[1]\n",
    "    \n",
    "    ###############################################\n",
    "    # ARCHITECTURE settings: found on MLP_SI\n",
    "    best_val_arch_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SI/XXX'\n",
    "    dd = np.load(best_val_arch_path_prefix + 'val_' + tuning_type + '.npz')\n",
    "    best_N_hl = int(dd['best_N_hl'])\n",
    "    best_N_hu = int(dd['best_N_hu'])\n",
    "    print \"\\tOptimal number of hidden layers / hidden units: \", best_N_hl, \" / \", best_N_hu\n",
    "    \n",
    "    # Iterate over subjects\n",
    "    for s, SID in enumerate(unique_srt_SIDs):\n",
    "\n",
    "        print SID\n",
    "\n",
    "        #######################\n",
    "        # Load the dataset split (for this SID); concat from both VIDs\n",
    "        ds1 = np.load('./../Dataset/'+TE_folder+'/Dataset_split/split_masks_' + SID  + 'Task2.npz')\n",
    "        ds2 = np.load('./../Dataset/'+TE_folder+'/Dataset_split/split_masks_' + SID  + 'Task3.npz')\n",
    "        train_mask = np.concatenate( (ds1['train_mask'], ds2['train_mask']) )\n",
    "        val_mask   = np.concatenate( (ds1['val_mask'],   ds2['val_mask']) )\n",
    "        test_mask  = np.concatenate( (ds1['test_mask'],  ds2['test_mask']) )\n",
    "\n",
    "        X_train = AF[train_mask]\n",
    "        X_val   = AF[val_mask]\n",
    "        X_test  = AF[test_mask]\n",
    "\n",
    "        Y_train = PF[train_mask]\n",
    "        Y_val   = PF[val_mask]\n",
    "        Y_test  = PF[test_mask]\n",
    "\n",
    "        train_batch_size = len(X_train)\n",
    "        val_batch_size = len(X_val)\n",
    "        test_batch_size = len(X_test)\n",
    "\n",
    "        ##########################\n",
    "        # Final train & test\n",
    "\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform', input_dim=N_features))\n",
    "        #model.add(Dropout(dropout))\n",
    "        for i in range(1, best_N_hl):\n",
    "            model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform'))\n",
    "            #model.add(Dropout(dropout))\n",
    "        model.add(Dense(N_targets, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "        #print model.summary()\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1) # stop after 10 epochs without improvement in val_acc\n",
    "\n",
    "        # Checkpoint model weights and the model itself: at each epoch\n",
    "        model_checkpoint_name = 'm_' + SID + '_{epoch:04d}_{loss:.4f}_{val_loss:.4f}.hdf5'\n",
    "        model_checkpoint = ModelCheckpoint(model_checkpoint_path_prefix + tuning_type + '/' + model_checkpoint_name, monitor='val_loss', \n",
    "                                           verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=epochs, batch_size=train_batch_size, \n",
    "                   validation_data = (X_val, Y_val), verbose=0, callbacks=[early_stop, model_checkpoint])\n",
    "        \n",
    "        ###############################################################################################################\n",
    "        # Evaluate on validation set\n",
    "        Y_val_pred = model.predict(X_val, batch_size=val_batch_size, verbose=0)\n",
    "\n",
    "        # Save results: predictions will be saved in radians; for generation on robot\n",
    "        # Raw and smoothed \n",
    "        val_VIDs = [SID + 'Task2', SID + 'Task3']\n",
    "        val_VIDs_ind_cnts = [len(ds1['val_mask']), len(ds2['val_mask'])]\n",
    "        SD_offsets = [len(ds1['train_mask']), len(ds2['train_mask']) ]\n",
    "\n",
    "        from postprocessingutils import save_predictions_and_eval\n",
    "        save_predictions_and_eval(save_results_path_prefix + 'MSvaltest_' + SID + '_' + tuning_type, \n",
    "                                 X_val, Y_val, Y_val_pred, 'MLP_SD', SEGMENT_LEN, val_VIDs, val_VIDs_ind_cnts, \n",
    "                                 SD_offsets)\n",
    "        \n",
    "        ###############################################################################################################\n",
    "        # Evaluate on testing set: only for 1_35_AF26\n",
    "        if tuning_type == '1_35_AF26':\n",
    "            Y_test_pred = model.predict(X_test, batch_size=test_batch_size, verbose=0)\n",
    "\n",
    "            # Save results: predictions will be saved in radians; for generation on robot\n",
    "            # Raw and smoothed \n",
    "            test_VIDs = [SID + 'Task2', SID + 'Task3']\n",
    "            test_VIDs_ind_cnts = [len(ds1['test_mask']), len(ds2['test_mask'])]\n",
    "            SD_offsets = [len(ds1['train_mask']) + len(ds1['val_mask']), len(ds2['train_mask']) + len(ds2['val_mask'])]\n",
    "\n",
    "            from postprocessingutils import save_predictions_and_eval\n",
    "            save_predictions_and_eval(save_results_path_prefix + 'MStest_' + SID + '_' + tuning_type, \n",
    "                                     X_test, Y_test, Y_test_pred, 'MLP_SD', SEGMENT_LEN, test_VIDs, test_VIDs_ind_cnts, \n",
    "                                     SD_offsets)\n",
    "\n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Re-Evaluate on VALIDATION & TEST SET using BEST MODEL\n",
    "# DONE\n",
    "#######################################################################################################\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "SEGMENT_LEN = 300 # for evaluation (local cca)\n",
    "\n",
    "tuning_types = [\n",
    "    '1_35_AF13',\n",
    "    '1_35_AF26', \n",
    "    '1_35_AF52', \n",
    "    '1_35_AF78'\n",
    "]\n",
    "AF_types = [\n",
    "    'AF_MFCC13_norm',\n",
    "    'AF_logFB26_norm',\n",
    "    'AF_logFB52_norm',\n",
    "    'AF_logFB78_norm'\n",
    "]\n",
    "\n",
    "np.random.seed(37) # for reproducibility\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SD/XXX'\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/MLP_SD/'\n",
    "\n",
    "unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs']\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "PF = np.load('./../Dataset/'+TE_folder+'/te_PF_smooth_LPBF_4.0.npz')['PF_smooth_LPBF']\n",
    "PF = PF[:, :11]\n",
    "N_targets = PF.shape[1]\n",
    "###########\n",
    "# Target (Y) normalisation, into range 0-1 according to constraints\n",
    "PF = norm_Y(PF)\n",
    "print \"Targets (Y) are TRANSFORMED to 0-1 range\"\n",
    "\n",
    "# Learning settings\n",
    "epochs = 1000\n",
    "dropout = 0.\n",
    "\n",
    "st = time.time()\n",
    "for tuning_type, AF_type in zip(tuning_types, AF_types):\n",
    "    \n",
    "    print TE_folder, AF_type, tuning_type\n",
    "    if tuning_type[-2:] != AF_type.split('_')[1][-2:]:\n",
    "        raise ValueError(\"Tuning type and audio feature type mismatch!\")\n",
    "        \n",
    "        \n",
    "    AF = np.load('./../Dataset/'+TE_folder+'/te_'+AF_type+'.npz')[AF_type]\n",
    "    N_features = AF.shape[1]\n",
    "    \n",
    "    # Iterate over subjects\n",
    "    for s, SID in enumerate(unique_srt_SIDs):\n",
    "\n",
    "        print SID\n",
    "\n",
    "        #######################\n",
    "        # Load the dataset split (for this SID); concat from both VIDs\n",
    "        ds1 = np.load('./../Dataset/'+TE_folder+'/Dataset_split/split_masks_' + SID  + 'Task2.npz')\n",
    "        ds2 = np.load('./../Dataset/'+TE_folder+'/Dataset_split/split_masks_' + SID  + 'Task3.npz')\n",
    "        val_mask   = np.concatenate( (ds1['val_mask'],   ds2['val_mask']) )\n",
    "        test_mask  = np.concatenate( (ds1['test_mask'],  ds2['test_mask']) )\n",
    "\n",
    "        X_val   = AF[val_mask]\n",
    "        X_test  = AF[test_mask]\n",
    "\n",
    "        Y_val   = PF[val_mask]\n",
    "        Y_test  = PF[test_mask]\n",
    "\n",
    "        val_batch_size = len(X_val)\n",
    "        test_batch_size = len(X_test)\n",
    "\n",
    "        #######################\n",
    "        # Load best model\n",
    "        model_name = sorted(glob.glob( model_checkpoint_path_prefix + tuning_type + '/m_' + SID + '_*' ))[-1] # take best model = last checkpointed\n",
    "        test_model_name = model_name\n",
    "        print \"Loading BEST model from:\", test_model_name\n",
    "        model = load_model(test_model_name)\n",
    "\n",
    "        ###############################################################################################################\n",
    "        # Evaluate on validation set\n",
    "        Y_val_pred = model.predict(X_val, batch_size=val_batch_size, verbose=0)\n",
    "\n",
    "        # Save results: predictions will be saved in radians; for generation on robot\n",
    "        # Raw and smoothed \n",
    "        val_VIDs = [SID + 'Task2', SID + 'Task3']\n",
    "        val_VIDs_ind_cnts = [len(ds1['val_mask']), len(ds2['val_mask'])]\n",
    "        SD_offsets = [len(ds1['train_mask']), len(ds2['train_mask']) ]\n",
    "\n",
    "        from postprocessingutils import save_predictions_and_eval\n",
    "        save_predictions_and_eval(save_results_path_prefix + 'MSBMvaltest_' + SID + '_' + tuning_type, \n",
    "                                 X_val, Y_val, Y_val_pred, 'MLP_SD', SEGMENT_LEN, val_VIDs, val_VIDs_ind_cnts, \n",
    "                                 SD_offsets)\n",
    "        \n",
    "        ###############################################################################################################\n",
    "        # Evaluate on testing set: only for 1_35_AF26\n",
    "        if tuning_type == '1_35_AF26':\n",
    "            Y_test_pred = model.predict(X_test, batch_size=test_batch_size, verbose=0)\n",
    "\n",
    "            # Save results: predictions will be saved in radians; for generation on robot\n",
    "            # Raw and smoothed \n",
    "            test_VIDs = [SID + 'Task2', SID + 'Task3']\n",
    "            test_VIDs_ind_cnts = [len(ds1['test_mask']), len(ds2['test_mask'])]\n",
    "            SD_offsets = [len(ds1['train_mask']) + len(ds1['val_mask']), len(ds2['train_mask']) + len(ds2['val_mask'])]\n",
    "\n",
    "            from postprocessingutils import save_predictions_and_eval\n",
    "            save_predictions_and_eval(save_results_path_prefix + 'MSBMtest_' + SID + '_' + tuning_type, \n",
    "                                     X_test, Y_test, Y_test_pred, 'MLP_SD', SEGMENT_LEN, test_VIDs, test_VIDs_ind_cnts, \n",
    "                                     SD_offsets)\n",
    "\n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Overall testing statistics over all subjects (subject-dependent case)\n",
    "# ON VALIDATION SET\n",
    "# DONE\n",
    "#######################################################################################################\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from geoutils import radToDeg\n",
    "from evalutils import show_test_results\n",
    "\n",
    "BM = True     # use best model, not last model\n",
    "# BM = False\n",
    "\n",
    "tuning_types = [\n",
    "    '1_35_AF13',\n",
    "    '1_35_AF26', \n",
    "    '1_35_AF52', \n",
    "    '1_35_AF78'\n",
    "]\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SD/XXX'\n",
    "\n",
    "unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs']\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "for i, tuning_type in enumerate(tuning_types):\n",
    "    print tuning_type\n",
    "    \n",
    "    rmse_overall_list = []\n",
    "    local_cca_YtYp = [] \n",
    "    local_cca_XYt = [] \n",
    "    local_cca_XYp = [] \n",
    "    jerkiness_true = []\n",
    "    jerkiness_pred = []\n",
    "    losses = []\n",
    "\n",
    "    for s, SID in enumerate(unique_srt_SIDs):\n",
    "\n",
    "        #print SID\n",
    "        if BM:\n",
    "            d = np.load(save_results_path_prefix + 'MSBMvaltest_' + SID + '_' + tuning_type + '.npz')\n",
    "        else:\n",
    "            d = np.load(save_results_path_prefix + 'MSvaltest_' + SID + '_' + tuning_type + '.npz')\n",
    "\n",
    "        ###############################################################################\n",
    "        # Show testing results: for raw Y and smoothed Y\n",
    "        #print \"===========================================Raw=====================\\n\"\n",
    "        #show_test_results(d['results_raw'])\n",
    "        #print \"===========================================Smooth=====================\\n\"\n",
    "        #show_test_results(d['results_smooth'])\n",
    "        #print \"================================================================\\n\"\n",
    "\n",
    "        ###############################################################################\n",
    "        # Plot predictions (post-smoothed and raw) against ground truths and audio \n",
    "    #     t_VID = 0 # test VID to show (only 0 or 1 in subject dependent case)\n",
    "    #     if t_VID >= len(d['Y_raw_list']):\n",
    "    #         raise ValueError(\"Required test VID is out of bounds!\")\n",
    "    #     Y_true = d['Y_true_list'][t_VID]\n",
    "    #     Y_raw = d['Y_raw_list'][t_VID]\n",
    "    #     Y_smooth = d['Y_smooth_list'][t_VID]\n",
    "    #     test_VID = d['test_VIDs'][t_VID]\n",
    "    #     SD_offset = d['SD_offsets'][t_VID]\n",
    "\n",
    "    #     plot_predictions(Y_true, Y_raw, Y_smooth, 'MLP_SD', angles_to_show='all', \n",
    "    #                          plot_start=SD_offset + 0, plot_length=300, input_mode='samples', SD_offset=SD_offset, \n",
    "    #                          test_VID=test_VID)\n",
    "\n",
    "        # Calculate overall measures\n",
    "        #print d['results_smooth'][1][0]\n",
    "        local_cca_YtYp.append( d['results_smooth'][3]['YtYp'] )\n",
    "        local_cca_XYt.append( d['results_smooth'][3]['XYt'] )\n",
    "        local_cca_XYp.append( d['results_smooth'][3]['XYp'] )\n",
    "        #print d['results_smooth'][3]['YtYp']\n",
    "        rmse_overall_list.append( d['results_smooth'][1][0] ) # take only means, not stds\n",
    "\n",
    "        jerkiness_true.append( np.sum(d['results_smooth'][4]['true']) )\n",
    "        jerkiness_pred.append( np.sum(d['results_smooth'][4]['pred']) )\n",
    "        \n",
    "        losses.append( d['test_loss'] )\n",
    "    \n",
    "    print \"======================OVERALL=====================\\n\"\n",
    "    print \"===========================================Smooth=====================\\n\"\n",
    "    print \"Mean RMSE_overall: \", np.mean(rmse_overall_list, axis=0), \"(deg)\"\n",
    "    print \n",
    "\n",
    "    print \"Local CCA XYt: \", np.mean(local_cca_XYt, axis=0)[0], \" +/- \", np.mean(local_cca_XYt, axis=0)[1]\n",
    "    print \"Local CCA XYp: \", np.mean(local_cca_XYp, axis=0)[0], \" +/- \", np.mean(local_cca_XYp, axis=0)[1]\n",
    "    print \"Local CCA delta\", abs( np.mean(local_cca_XYt, axis=0)[0] - np.mean(local_cca_XYp, axis=0)[0] )\n",
    "    print \"Local CCA YtYp: \", np.mean(local_cca_YtYp, axis=0)[0], \" +/- \", np.mean(local_cca_YtYp, axis=0)[1]\n",
    "    print \n",
    "    print \"Jerkiness (true): \", np.mean(jerkiness_true, axis=0)    # mean per subject\n",
    "    print \"Jerkiness (pred): \", np.mean(jerkiness_pred, axis=0)\n",
    "    print \n",
    "    print \"Loss:\", np.mean(losses)\n",
    "    print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Overall testing statistics over all subjects (subject-dependent case)\n",
    "# DONE\n",
    "#######################################################################################################\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from geoutils import radToDeg\n",
    "from evalutils import show_test_results\n",
    "\n",
    "tuning_type = '1_35_AF26'\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/MLP_SD/XXX'\n",
    "\n",
    "unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs']\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "print tuning_type\n",
    "\n",
    "rmse_overall_list = []\n",
    "local_cca_YtYp = [] \n",
    "local_cca_XYt = [] \n",
    "local_cca_XYp = [] \n",
    "jerkiness_true = []\n",
    "jerkiness_pred = []\n",
    "losses = []\n",
    "\n",
    "for s, SID in enumerate(unique_srt_SIDs):\n",
    "\n",
    "    #print SID\n",
    "    d = np.load(save_results_path_prefix + 'MSBMtest_' + SID + '_' + tuning_type + '.npz')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Show testing results: for raw Y and smoothed Y\n",
    "    #print \"===========================================Raw=====================\\n\"\n",
    "    #show_test_results(d['results_raw'])\n",
    "    #print \"===========================================Smooth=====================\\n\"\n",
    "    #show_test_results(d['results_smooth'])\n",
    "    #print \"================================================================\\n\"\n",
    "\n",
    "    ###############################################################################\n",
    "    # Plot predictions (post-smoothed and raw) against ground truths and audio \n",
    "#     t_VID = 0 # test VID to show (only 0 or 1 in subject dependent case)\n",
    "#     if t_VID >= len(d['Y_raw_list']):\n",
    "#         raise ValueError(\"Required test VID is out of bounds!\")\n",
    "#     Y_true = d['Y_true_list'][t_VID]\n",
    "#     Y_raw = d['Y_raw_list'][t_VID]\n",
    "#     Y_smooth = d['Y_smooth_list'][t_VID]\n",
    "#     test_VID = d['test_VIDs'][t_VID]\n",
    "#     SD_offset = d['SD_offsets'][t_VID]\n",
    "\n",
    "#     plot_predictions(Y_true, Y_raw, Y_smooth, 'MLP_SD', angles_to_show='all', \n",
    "#                          plot_start=SD_offset + 0, plot_length=300, input_mode='samples', SD_offset=SD_offset, \n",
    "#                          test_VID=test_VID)\n",
    "\n",
    "    # Calculate overall measures\n",
    "    #print d['results_smooth'][1][0]\n",
    "    local_cca_YtYp.append( d['results_smooth'][3]['YtYp'] )\n",
    "    local_cca_XYt.append( d['results_smooth'][3]['XYt'] )\n",
    "    local_cca_XYp.append( d['results_smooth'][3]['XYp'] )\n",
    "    #print d['results_smooth'][3]['YtYp']\n",
    "    rmse_overall_list.append( d['results_smooth'][1][0] ) # take only means, not stds\n",
    "\n",
    "    jerkiness_true.append( np.sum(d['results_smooth'][4]['true']) )\n",
    "    jerkiness_pred.append( np.sum(d['results_smooth'][4]['pred']) )\n",
    "\n",
    "    losses.append( d['test_loss'] )\n",
    "\n",
    "print \"======================OVERALL=====================\\n\"\n",
    "print \"===========================================Smooth=====================\\n\"\n",
    "print \"Mean RMSE_overall: \", np.mean(rmse_overall_list, axis=0), \"(deg)\"\n",
    "print \n",
    "\n",
    "print \"Local CCA XYt: \", np.mean(local_cca_XYt, axis=0)[0], \" +/- \", np.mean(local_cca_XYt, axis=0)[1]\n",
    "print \"Local CCA XYp: \", np.mean(local_cca_XYp, axis=0)[0], \" +/- \", np.mean(local_cca_XYp, axis=0)[1]\n",
    "print \"Local CCA delta\", abs( np.mean(local_cca_XYt, axis=0)[0] - np.mean(local_cca_XYp, axis=0)[0] )\n",
    "print \"Local CCA YtYp: \", np.mean(local_cca_YtYp, axis=0)[0], \" +/- \", np.mean(local_cca_YtYp, axis=0)[1]\n",
    "print \n",
    "print \"Jerkiness (true): \", np.mean(jerkiness_true, axis=0)    # mean per subject\n",
    "print \"Jerkiness (pred): \", np.mean(jerkiness_pred, axis=0)\n",
    "print \"Jerkiness (delta): \", abs( np.mean(jerkiness_pred, axis=0) - np.mean(jerkiness_true, axis=0) )\n",
    "print \n",
    "print \"Loss:\", np.mean(losses)\n",
    "print \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the best model for each subject\n",
    "# For MLP SD\n",
    "# DONE\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from shutil import copy2\n",
    "\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/MLP_SD/'\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "unique_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_unique_srt_VIDs.npz')['unique_srt_VIDs']\n",
    "all_srt_VIDs = np.load('./../Dataset/'+TE_folder+'/te_VIDs.npz')['VIDs']\n",
    "unique_srt_SIDs = np.array([x[:5] for i, x in enumerate(unique_srt_VIDs) if i % 2 == 0]) # ['PID02', 'PID05', ..\n",
    "\n",
    "tuning_types = [\n",
    "    '1_35_AF13',\n",
    "    '1_35_AF26', \n",
    "    '1_35_AF52', \n",
    "    '1_35_AF78'\n",
    "]\n",
    "\n",
    "for tuning_type in tuning_types:\n",
    "\n",
    "    for s, SID in enumerate(unique_srt_SIDs):\n",
    "        test_model_name = sorted(glob.glob(model_checkpoint_path_prefix + tuning_type + '/m_' + SID + '_*'))[-1]   \n",
    "        new_name = model_checkpoint_path_prefix + tuning_type + '_' + test_model_name.split('/')[-1]\n",
    "        copy2(test_model_name, new_name)\n",
    "        print new_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
