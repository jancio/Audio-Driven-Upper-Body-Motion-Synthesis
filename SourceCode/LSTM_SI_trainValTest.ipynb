{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Audio-driven upper-body motion synthesis on a humanoid robot\n",
    "# Computer Science Tripos Part III Project\n",
    "# Jan Ondras (jo356@cam.ac.uk), Trinity College, University of Cambridge\n",
    "# 2017/18\n",
    "#####################################################################################\n",
    "# Training, validation and testing of the LSTM-SI model\n",
    "# (for various dropout probabilities)\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train VIDs: ['PID05Task2' 'PID05Task3' 'PID16Task2' 'PID16Task3' 'PID06Task2'\n",
      " 'PID06Task3' 'PID09Task2' 'PID09Task3' 'PID22Task2' 'PID22Task3'\n",
      " 'PID15Task2' 'PID15Task3' 'PID02Task2' 'PID02Task3' 'PID13Task2'\n",
      " 'PID13Task3' 'PID21Task2' 'PID21Task3' 'PID26Task2' 'PID26Task3'\n",
      " 'PID08Task2' 'PID08Task3' 'PID17Task2' 'PID17Task3' 'PID11Task2'\n",
      " 'PID11Task3' 'PID10Task2' 'PID10Task3' 'PID24Task2' 'PID24Task3']\n",
      "Val VIDs: ['PID25Task2' 'PID25Task3' 'PID20Task2' 'PID20Task3']\n",
      "Test VIDs: ['PID18Task2' 'PID18Task3' 'PID23Task2' 'PID23Task3'] \n",
      "\n",
      "Data were loaded.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# LSTM Training, cross-validation and testing\n",
    "# SUBJECT INDEPENDENT\n",
    "###############################################################################################################\n",
    "# Load segmented data, already split\n",
    "# NOT Rescale target angles to range [0,1] (already done when segmenting)\n",
    "# NOT: z-norm audio features, subject-independently\n",
    "###############################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "AF_type = 'AF_logFB26_norm'\n",
    "# AF_type = 'AF_MFCC13_norm'\n",
    "\n",
    "if AF_type == 'AF_logFB26_norm':\n",
    "    SEG_folder = 'Segments_logFB26'\n",
    "elif AF_type == 'AF_MFCC13_norm':\n",
    "    SEG_folder = 'Segments_MFCC13'\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "\n",
    "# Load segmented data, already split\n",
    "X_train = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_train.npz')['X'] \n",
    "Y_train = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_train.npz')['Y'] \n",
    "\n",
    "X_val = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val.npz')['X'] \n",
    "Y_val = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val.npz')['Y'] \n",
    "\n",
    "X_val_RT = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val_RT.npz')['X'] \n",
    "Y_val_RT = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val_RT.npz')['Y'] \n",
    "\n",
    "X_test = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test.npz')['X'] \n",
    "Y_test = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test.npz')['Y'] \n",
    "\n",
    "X_test_RT = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test_RT.npz')['X'] \n",
    "Y_test_RT = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test_RT.npz')['Y'] \n",
    "\n",
    "train_VIDs = np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_train.npz')['train_VIDs']\n",
    "val_VIDs =   np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val.npz')['val_VIDs']\n",
    "test_VIDs =  np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test.npz')['test_VIDs']\n",
    "print \"Train VIDs:\", train_VIDs\n",
    "print \"Val VIDs:\", val_VIDs\n",
    "print \"Test VIDs:\", test_VIDs, \"\\n\"\n",
    "\n",
    "N_train_seg =    np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_train.npz')['N_train_seg']\n",
    "N_val_seg =      np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val.npz')['N_val_seg']\n",
    "N_val_RT_seg =  np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_val_RT.npz')['N_val_RT_seg']\n",
    "N_test_seg =     np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test.npz')['N_test_seg']\n",
    "N_test_RT_seg =  np.load('./../Dataset/'+TE_folder+'/' + SEG_folder + '/seg_test_RT.npz')['N_test_RT_seg']\n",
    "\n",
    "print \"Data were loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# ARCHITECTURE TUNING\n",
    "# TRAIN & VALIDATE\n",
    "# DONE\n",
    "######################################################################################################\n",
    "# Sequence tagging approach (LSTMs)\n",
    " # if all feature values at a timestep are equal to mask_value=0., then the timestep is skipped\n",
    "\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Masking, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "rand_seed = 37 \n",
    "np.random.seed(rand_seed) # for reproducibility\n",
    "   \n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/LSTM_SI/'\n",
    "save_training_hist_path_prefix = './../Dataset/'+TE_folder+'/Results/LSTM_SI/trainHistES_' # ES = early stop\n",
    "\n",
    "N_runs = 10\n",
    "N_runs = 1\n",
    "SEGMENT_LEN = 300 # == X_train.shape[1]\n",
    "# useDropout = True\n",
    "useDropout = False\n",
    "# dropouts = [0.5, 0.5]\n",
    "N_epochs = 100                    # this value is saved as N_epochs\n",
    "train_batch_size = 15000 # same for all models; for LOGFB26 and also for MFCC13\n",
    "N_features = X_train.shape[2]\n",
    "N_targets  = Y_train.shape[2]\n",
    "\n",
    "N_LSTM_units_range = [3, 6, 9, 12, 15, 18, 21, 24, 27]\n",
    "\n",
    "st = time.time()\n",
    "for N_LSTM_units in N_LSTM_units_range:\n",
    "\n",
    "    print(\"Segment length: {:d};\\n Train batch size: {:d};\\n LSTM units: {:d};\\n Max epochs: {:d};\\n #features: {:d};\\n #targets: {:d};\\n #runs: {:d};\"\n",
    "          .format(SEGMENT_LEN, train_batch_size, N_LSTM_units, N_epochs, N_features, N_targets, N_runs))\n",
    "\n",
    "    # SET MODEL TYPE\n",
    "    model_type = '{:d}_{:d}_{:02d}'.format(N_runs, N_features, N_LSTM_units)\n",
    "    if useDropout:\n",
    "        model_type = model_type + '_DROP_{:.2f}_{:.2f}'.format(dropouts[0], dropouts[1])\n",
    "    print \"MODEL TYPE:\\t\\t\", model_type\n",
    "\n",
    "    # Create LSTM model\n",
    "    model = Sequential()\n",
    "    model.add( Masking(mask_value=0., input_shape=(SEGMENT_LEN, N_features)) )\n",
    "    if useDropout:\n",
    "        model.add( Dropout(dropouts[0]) )\n",
    "    model.add( LSTM(N_LSTM_units, return_sequences=True) )\n",
    "    if useDropout:\n",
    "        model.add( Dropout(dropouts[1]) )    \n",
    "    model.add( TimeDistributed(Dense(N_targets, activation='sigmoid')) )\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print model.summary()\n",
    "\n",
    "    # Set early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "    # Checkpoint model weights and the model itself: at each epoch\n",
    "    #model_checkpoint_name = 'm_{epoch:04d}_{loss:.4f}_{val_loss:.4f}.hdf5'\n",
    "    #model_checkpoint = ModelCheckpoint(model_checkpoint_path_prefix + model_checkpoint_name, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for i in range(N_runs):\n",
    "        # Tain & validate\n",
    "        hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=N_epochs, shuffle=True,  \n",
    "                         batch_size=train_batch_size, verbose=1, \n",
    "                         #callbacks=[early_stop, model_checkpoint]\n",
    "                         callbacks=[early_stop]\n",
    "                        )\n",
    "        loss.append( hist.history['loss'] )\n",
    "        val_loss.append( hist.history['val_loss'] )\n",
    "    # Save training history\n",
    "    np.savez(save_training_hist_path_prefix + model_type + '.npz', \n",
    "            loss=loss, val_loss=val_loss, \n",
    "            N_params=model.count_params(), train_batch_size=train_batch_size, \n",
    "            N_runs=N_runs, N_LSTM_units=N_LSTM_units, N_epochs=N_epochs)\n",
    "    print \"Saved training history.\"\n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# TRAIN & SAVE best MODELS, also see effect of dropout; for best N_LSTM_units = 12\n",
    "# 3 models\n",
    "# DONE\n",
    "######################################################################################################\n",
    "# Sequence tagging approach (LSTMs)\n",
    " # if all feature values at a timestep are equal to mask_value=0., then the timestep is skipped\n",
    "      \n",
    "import time\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Masking, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "rand_seed = 37 \n",
    "np.random.seed(rand_seed) # for reproducibility\n",
    "\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/LSTM_SI/'\n",
    "save_training_hist_path_prefix = './../Dataset/'+TE_folder+'/Results/LSTM_SI/trainHistES_' # ES = early stop\n",
    "\n",
    "N_runs = 1\n",
    "SEGMENT_LEN = 300 # == X_train.shape[1]\n",
    "dropouts_range = [[0., 0.], [0.25, 0.25], [0.5, 0.5]]\n",
    "N_epochs = 100                    # this value is saved as N_epochs\n",
    "train_batch_size = 15000 # same for all models; for LOGFB26 and also for MFCC13\n",
    "N_features = X_train.shape[2]\n",
    "N_targets  = Y_train.shape[2]\n",
    "\n",
    "N_LSTM_units = 12 # BEST FOUND in LSTM_SI\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for dropouts in dropouts_range:\n",
    "    \n",
    "    if dropouts[0] == 0. and dropouts[1] == 0.:\n",
    "        useDropout = False\n",
    "    else:\n",
    "        useDropout = True\n",
    "\n",
    "    print(\"Segment length: {:d};\\n Train batch size: {:d};\\n LSTM units: {:d};\\n Max epochs: {:d};\\n #features: {:d};\\n #targets: {:d};\\n #runs: {:d};\"\n",
    "          .format(SEGMENT_LEN, train_batch_size, N_LSTM_units, N_epochs, N_features, N_targets, N_runs))\n",
    "\n",
    "    # SET MODEL TYPE\n",
    "    model_type = '{:d}_{:d}_{:02d}'.format(N_runs, N_features, N_LSTM_units)\n",
    "    model_type = model_type + '_DROP_{:.2f}_{:.2f}'.format(dropouts[0], dropouts[1])\n",
    "    print \"MODEL TYPE:\\t\\t\", model_type\n",
    "    if not os.path.isdir(model_checkpoint_path_prefix + model_type):\n",
    "        os.mkdir(model_checkpoint_path_prefix + model_type)\n",
    "\n",
    "    # Create LSTM model\n",
    "    model = Sequential()\n",
    "    model.add( Masking(mask_value=0., input_shape=(SEGMENT_LEN, N_features)) )\n",
    "    if useDropout:\n",
    "        model.add( Dropout(dropouts[0]) )\n",
    "    model.add( LSTM(N_LSTM_units, return_sequences=True) )\n",
    "    if useDropout:\n",
    "        model.add( Dropout(dropouts[1]) )    \n",
    "    model.add( TimeDistributed(Dense(N_targets, activation='sigmoid')) )\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print model.summary()\n",
    "\n",
    "    # Set early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "    # Checkpoint model weights and the model itself: at each epoch\n",
    "    model_checkpoint_name = 'm_{epoch:04d}_{loss:.4f}_{val_loss:.4f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_checkpoint_path_prefix + model_type + '/' + model_checkpoint_name, monitor='val_loss', verbose=1, \n",
    "                                       save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for i in range(N_runs):\n",
    "        # Tain & validate\n",
    "        hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=N_epochs, shuffle=True,  \n",
    "                         batch_size=train_batch_size, verbose=1, \n",
    "                         callbacks=[early_stop, model_checkpoint]\n",
    "                        )\n",
    "        loss.append( hist.history['loss'] )\n",
    "        val_loss.append( hist.history['val_loss'] )\n",
    "    # Save training history\n",
    "    np.savez(save_training_hist_path_prefix + model_type + '.npz', \n",
    "            loss=loss, val_loss=val_loss, \n",
    "            N_params=model.count_params(), train_batch_size=train_batch_size, \n",
    "            N_runs=N_runs, N_LSTM_units=N_LSTM_units, N_epochs=N_epochs)\n",
    "    print \"Saved training history: \", save_training_hist_path_prefix + model_type + '.npz'\n",
    "    print \"Best model saved to: \", model_checkpoint_path_prefix + model_type + '/' + model_checkpoint_name\n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Evaluate 3 models from above on VALIDATION & TEST SET;\n",
    "# save results\n",
    "# DONE\n",
    "######################################################################################################\n",
    "from postprocessingutils import save_predictions_and_eval\n",
    "from keras.models import load_model\n",
    "import glob\n",
    "\n",
    "model_checkpoint_path_prefix = './ModelCheckpoints/LSTM_SI/'\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/LSTM_SI/'\n",
    "\n",
    "N_runs = 1\n",
    "SEGMENT_LEN = 300 # == X_train.shape[1]\n",
    "dropouts_range = [[0., 0.], [0.25, 0.25], [0.5, 0.5]]\n",
    "N_epochs = 100                    # this value is saved as N_epochs\n",
    "train_batch_size = 15000 # same for all models; for LOGFB26 and also for MFCC13\n",
    "N_features = X_train.shape[2]\n",
    "\n",
    "N_LSTM_units = 12 # BEST FOUND in LSTM_SI\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for dropouts in dropouts_range:\n",
    "    \n",
    "    if dropouts[0] == 0. and dropouts[1] == 0.:\n",
    "        useDropout = False\n",
    "    else:\n",
    "        useDropout = True\n",
    "\n",
    "    # SET MODEL TYPE\n",
    "    model_type = '{:d}_{:d}_{:02d}_DROP_{:.2f}_{:.2f}'.format(N_runs, N_features, N_LSTM_units, dropouts[0], dropouts[1])\n",
    "    print \"MODEL TYPE:\\t\\t\", model_type\n",
    "    \n",
    "    #########################################\n",
    "    # Load trained model\n",
    "    test_model_name = sorted(glob.glob(model_checkpoint_path_prefix + model_type + '/m_*'))[-1]\n",
    "    print \"Loaded model:\", test_model_name\n",
    "    model = load_model( test_model_name )\n",
    "    N_epochs = int( (test_model_name.split('/')[-1]).split('_')[1] )\n",
    "\n",
    "    ###############################################################################################################\n",
    "    # Evaluate on validation set\n",
    "    # ONLINE TESTING (as if new timesteps arrive one-by-one)\n",
    "    \n",
    "    val_batch_size = X_val_RT.shape[0]\n",
    "    Y_val_pred = model.predict(X_val_RT, batch_size=val_batch_size, verbose=1)  \n",
    "    Y_val_pred = Y_val_pred[:, -1, :] # last item from each segment is the (ONLINE) final prediction\n",
    "    Y_val_true = Y_val_RT[:, -1, :]\n",
    "    X_val_RT_last = X_val_RT[:, -1, :]\n",
    "    print X_val_RT.shape, X_val_RT_last.shape, Y_val_pred.shape, Y_val_true.shape\n",
    "    \n",
    "    # Save results: predictions will be saved in radians; for generation on robot\n",
    "    # Raw and smoothed (low-pass 4Hz)\n",
    "    from postprocessingutils import save_predictions_and_eval\n",
    "    save_predictions_and_eval(save_results_path_prefix + 'MSBMvaltest_' + model_type, \n",
    "                     X_val_RT_last, Y_val_true, Y_val_pred, 'LSTM_SI', SEGMENT_LEN, val_VIDs, N_val_RT_seg, \n",
    "                             N_params=model.count_params(), N_epochs=N_epochs  )\n",
    "    \n",
    "    ###############################################################################################################\n",
    "    # Evaluate on testing set: \n",
    "    # ONLINE TESTING (as if new timesteps arrive one-by-one)\n",
    "    \n",
    "    test_batch_size = X_test_RT.shape[0]\n",
    "    Y_test_pred = model.predict(X_test_RT, batch_size=test_batch_size, verbose=1)\n",
    "    Y_test_pred = Y_test_pred[:, -1, :] # last item from each segment is the (ONLINE) final prediction\n",
    "    Y_test_true = Y_test_RT[:, -1, :]\n",
    "    X_test_RT_last = X_test_RT[:, -1, :]\n",
    "    print X_test_RT.shape, X_test_RT_last.shape, Y_test_pred.shape, Y_test_true.shape\n",
    "\n",
    "    # Save results: predictions will be saved in radians; for generation on robot\n",
    "    # Raw and smoothed (low-pass 4Hz)\n",
    "    from postprocessingutils import save_predictions_and_eval\n",
    "    save_predictions_and_eval(save_results_path_prefix + 'MSBMtest_' + model_type, \n",
    "                     X_test_RT_last, Y_test_true, Y_test_pred, 'LSTM_SI', SEGMENT_LEN, test_VIDs, N_test_RT_seg, \n",
    "                             N_params=model.count_params(), N_epochs=N_epochs )       \n",
    "    \n",
    "    print \"\\tTime taken: \", time.time()-st, (time.time()-st)/60.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# OPTIONAL: Load already trained model?\n",
    "##################################################################################################\n",
    "from keras.models import load_model\n",
    "test_model_name = './ModelCheckpoints/LSTM_SI/m_0045_0.0165_0.0185.hdf5'\n",
    "model = load_model(test_model_name)\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Testing (on RT segments, always): offline test (low-pass filt); online test (Kalman filter)\n",
    "###############################################################################################################\n",
    "\n",
    "from evalutils import eval_test, plot_predictions, inv_norm_Y\n",
    "\n",
    "# ONLINE TESTING (as if new timesteps arrive one-by-one)\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/LSTM_SI/'\n",
    "test_batch_size = X_test_RT.shape[0]\n",
    "\n",
    "# Predict on test set\n",
    "Y_test_pred = model.predict(X_test_RT, batch_size=test_batch_size, verbose=1)\n",
    "Y_test_pred = Y_test_pred[:, -1, :] # last item from each segment is the (ONLINE) final prediction\n",
    "Y_test_true = Y_test_RT[:, -1, :]\n",
    "X_test_RT_last = X_test_RT[:, -1, :]\n",
    "print X_test_RT.shape, X_test_RT_last.shape, Y_test_pred.shape, Y_test_true.shape\n",
    "\n",
    "###############################################################################################################\n",
    "# Save results: predictions will be saved in radians; for generation on robot\n",
    "# Raw and smoothed (LPBF_4)\n",
    "from postprocessingutils import save_predictions_and_eval\n",
    "save_predictions_and_eval(save_results_path_prefix + 'test_1', \n",
    "                 X_test_RT_last, Y_test_true, Y_test_pred, 'LSTM_SI', SEGMENT_LEN, test_VIDs, N_test_RT_seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Show testing results\n",
    "###############################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from evalutils import show_test_results, plot_predictions\n",
    "\n",
    "TE_folder = 'TrainingExamples_16kHz'\n",
    "save_results_path_prefix = './../Dataset/'+TE_folder+'/Results/LSTM_SI/'\n",
    "d = np.load(save_results_path_prefix + 'test_1.npz')\n",
    "\n",
    "###############################################################################\n",
    "# Show testing results: for raw Y and smoothed Y\n",
    "print \"===========================================\\nRaw=====================\\n\"\n",
    "show_test_results(d['results_raw'])\n",
    "print \"===========================================\\nSmooth=====================\\n\"\n",
    "show_test_results(d['results_smooth'])\n",
    "print \"================================================================\\n\"\n",
    "\n",
    "###############################################################################\n",
    "# Plot predictions (post-smoothed and raw) against ground truths and audio \n",
    "t_VID = 3 # test VID to show\n",
    "if t_VID >= len(d['Y_raw_list']):\n",
    "    raise ValueError(\"Required test VID is out of bounds!\")\n",
    "Y_true = d['Y_true_list'][t_VID]\n",
    "Y_raw = d['Y_raw_list'][t_VID]\n",
    "Y_smooth = d['Y_smooth_list'][t_VID]\n",
    "test_VID = d['test_VIDs'][t_VID]\n",
    "\n",
    "plot_predictions(Y_true, Y_raw, Y_smooth, 'LSTM_SI', angles_to_show='all', \n",
    "                     plot_start=1.0, plot_length=3.0, input_mode='time', SD_offset=None, \n",
    "                     test_VID=test_VID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
