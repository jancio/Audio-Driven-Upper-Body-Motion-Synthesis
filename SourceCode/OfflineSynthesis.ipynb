{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Audio-driven upper-body motion synthesis on a humanoid robot\n",
    "# Computer Science Tripos Part III Project\n",
    "# Jan Ondras (jo356@cam.ac.uk), Trinity College, University of Cambridge\n",
    "# 2017/18\n",
    "#####################################################################################\n",
    "# 1. cell) given audio file use pose regression model to predict (and save) movements \n",
    "# 2. cell) perform OFFLINE synthesis using these predictions\n",
    "\n",
    "# Before starting the Jupyter Notebook you may need to run the following from the directory you start the Jupyter Notebook: \n",
    "# export PYTHONPATH=${PYTHONPATH}:~/Desktop/pynaoqi-python2.7-2.5.5.5-linux64/lib/python2.7/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# 1.) Make pose predictions\n",
    "#     - Extract audio features\n",
    "#     - z-normalise\n",
    "#     - Load model\n",
    "#     - Predict\n",
    "#######################################################################################################\n",
    "\n",
    "#######################################\n",
    "# BEGIN SETTINGS\n",
    "\n",
    "# Input audio\n",
    "AUDIO_INPUT = './Demo/audio.wav'\n",
    "\n",
    "# Select pose regression model to use\n",
    "model_type = 'MLP_SI'\n",
    "model_type = 'LSTM_SI'\n",
    "\n",
    "# Where to save predictions (to be used by cell 2 for synthesis)\n",
    "predictions_path = './Demo/predictions' # file extension .npz added automatically\n",
    "\n",
    "# END SETTINGS\n",
    "######################################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from postprocessingutils import save_predictions_and_eval_wo_truth_TTS\n",
    "from keras.models import load_model\n",
    "from python_speech_features import mfcc, delta, logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "\n",
    "AUDIO_RATE = 16000 # Hz\n",
    "SEGMENT_LEN = 300 # equivalent to N_tau\n",
    "\n",
    "model_path_prefix = './../Models/'\n",
    "test_model_name = model_path_prefix + model_type + '/' + model_type + '_model.hdf5'\n",
    "\n",
    "##########################################################################################\n",
    "# Load audio data\n",
    "print \"Using audio: \", AUDIO_INPUT\n",
    "(rate, audio_data) = wav.read(AUDIO_INPUT)\n",
    "if rate != AUDIO_RATE:\n",
    "    raise ValueError('Check audio rate!')\n",
    "\n",
    "##############################\n",
    "# Extract audio features\n",
    "audio_features = logfbank(audio_data,samplerate=AUDIO_RATE,winlen=0.025,winstep=0.01,\n",
    "                             nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97)\n",
    "print \"Shape of audio_features: \", audio_features.shape\n",
    "\n",
    "##############################\n",
    "# Z-normalisation\n",
    "# Z-norm just this set: i.e. subject standardisation, as it was done for original dataset\n",
    "audio_features = StandardScaler().fit_transform(audio_features)\n",
    "\n",
    "##############################\n",
    "# Create segments: audio_features->X_test\n",
    "# for realtime testing: zero-pad segments at the beginning: #segments=#frames\n",
    "if model_type == 'LSTM_SI':\n",
    "    N_audio_frames = len(audio_features)\n",
    "    N_features = audio_features.shape[1]\n",
    "    X_test = np.zeros((N_audio_frames, SEGMENT_LEN, N_features)) # #segments=#frames\n",
    "    for j in range(N_audio_frames):\n",
    "        # Do zero-padding at the beginning\n",
    "        if j < SEGMENT_LEN - 1:\n",
    "            X_test[j, SEGMENT_LEN - j - 1:] = audio_features[:j+1]\n",
    "        # Otherwise: as in the above section\n",
    "        else:\n",
    "            X_test[j] = audio_features[j-SEGMENT_LEN+1:j+1]\n",
    "else:\n",
    "    X_test = audio_features\n",
    "print X_test.shape\n",
    "\n",
    "##############################\n",
    "# Load model\n",
    "print \"Model:\", test_model_name\n",
    "model = load_model(test_model_name)\n",
    "print model.summary()\n",
    "\n",
    "##############################\n",
    "# Predict\n",
    "test_batch_size = X_test.shape[0]\n",
    "Y_pred = model.predict(X_test, batch_size=test_batch_size, verbose=1)\n",
    "if model_type == 'LSTM_SI':\n",
    "    Y_pred = Y_pred[:, -1, :] # last item from each segment is the (ONLINE) final prediction\n",
    "\n",
    "print \"Shape of predictions: \", Y_pred.shape\n",
    "\n",
    "###############################################################################################################\n",
    "# Save results: predictions will be saved in radians; for generation on robot\n",
    "# Raw and smoothed (LPBF_4)\n",
    "# also Evaluate predictions: local CCA, jerkiness, & plots\n",
    "save_predictions_and_eval_wo_truth_TTS(predictions_path, \n",
    "                                   audio_features, Y_pred, SEGMENT_LEN\n",
    "                                  )\n",
    "print \"Predictions saved, smoothed, and evaluated (local CCA, jerkiness).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predictions from:  ./Demo/predictions.npz\n",
      "Synthesising smoothed movements.\n",
      "Audio playback started.\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "# 2.) Simulate pose on robot (using above predictions)\n",
    "\n",
    "# Before starting\n",
    "# 1. run NaoQi framework\n",
    "# 2. connect to the real robot OR start Choregraphe simulator and connect to the virtual robot\n",
    "#######################################################################################################\n",
    "\n",
    "#######################################\n",
    "# BEGIN SETTINGS\n",
    "\n",
    "# Simulate post-smoothed movements (or raw)\n",
    "smoothPredictions = True\n",
    "# smoothPredictions = False\n",
    "\n",
    "audioOnly = True    # only audio playback is run\n",
    "# audioOnly = False # audio-visual playback is run, if video file is provided\n",
    "\n",
    "AUDIO_PLAYBACK_FILE = './Demo/audio.wav'\n",
    "AUDIOVISUAL_PLAYBACK_FILE = './Demo/video.mp4'\n",
    "\n",
    "# Path to predictions obtained in cell 1\n",
    "predictions_path = './Demo/predictions.npz'\n",
    "\n",
    "# Specify IP and port to connect to the robot\n",
    "IP = \"127.0.0.1\"\n",
    "port = 36571\n",
    "IP = 'pepper.local' # real robot settings\n",
    "port = 9559\n",
    "\n",
    "# END SETTINGS\n",
    "######################################\n",
    "\n",
    "from naoqi import ALProxy\n",
    "import numpy as np\n",
    "import time\n",
    "from subprocess import Popen\n",
    "import os\n",
    "\n",
    "FR = 100. # frame rate of pose features\n",
    "dt = 1./FR\n",
    "angles_names = [\n",
    "    \"HeadPitch\", \"HeadYaw\", \n",
    "    \"LShoulderRoll\", \"LShoulderPitch\", \"LElbowRoll\", \"LElbowYaw\",\n",
    "    \"RShoulderRoll\", \"RShoulderPitch\", \"RElbowRoll\", \"RElbowYaw\", \n",
    "    \"HipRoll\"\n",
    "]\n",
    "N_targets = len(angles_names)\n",
    "angles_used_i = np.arange(N_targets)\n",
    "\n",
    "# Connect to the robot\n",
    "motionProxy = ALProxy(\"ALMotion\", IP, port)\n",
    "\n",
    "# Load predictions\n",
    "print \"Using predictions from: \", predictions_path\n",
    "if smoothPredictions:\n",
    "    print \"Synthesising smoothed movements.\"\n",
    "    Y_pred = np.load(predictions_path)['Y_smooth']\n",
    "else:\n",
    "    print \"Synthesising non-smoothed movements.\"\n",
    "    Y_pred = np.load(predictions_path)['Y_raw']\n",
    "        \n",
    "# Reset robot to neutral pose\n",
    "for an in angles_names:\n",
    "    angle_reset = 0.\n",
    "    if an == 'LShoulderPitch' or an == 'RShoulderPitch':\n",
    "        angle_reset = angle_reset + np.pi/2\n",
    "    motionProxy.setAngles(an, angle_reset, 1.)\n",
    "\n",
    "# Run audio/video simultaneously\n",
    "if audioOnly:\n",
    "    print \"Audio playback started.\"\n",
    "    Popen([\"vlc\", \"--play-and-exit\", AUDIO_PLAYBACK_FILE])  # non-blocking\n",
    "else:\n",
    "    print \"Audio-visual playback started.\"\n",
    "    Popen([\"vlc\", \"--play-and-exit\", AUDIOVISUAL_PLAYBACK_FILE])  # non-blocking\n",
    "        \n",
    "# Synthesise\n",
    "st = time.time()\n",
    "for frame_i, angles_vector in enumerate(Y_pred):\n",
    "    for angle_i in angles_used_i:\n",
    "        motionProxy.setAngles(angles_names[angle_i], angles_vector[angle_i], 1.)\n",
    "    \n",
    "    # Adaptive time synchronisation between robot pose generation and audio/video\n",
    "    adaptive_dt = ((frame_i+1.) * dt - time.time() + st)\n",
    "    if adaptive_dt > 0.:\n",
    "        time.sleep(0.95*adaptive_dt)\n",
    "        \n",
    "et = time.time()\n",
    "print \"\\tTotal simulation time: \", et - st, \" s = \", (et - st)/60., \" min\"\n",
    "print \"===================================================================================\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
